---
title: "User guide"
format: 
  html:
    page-layout: full
    output-file: "User_guide"
    output-ext:  "html"
author:
  - name: Alice Trevail
    orcid: 0000-0002-6459-5213
  - name: Stephen Lang
    orcid: 0000-0001-5820-4346
  - name: Luke Ozsanlav-Harris
    orcid: 0000-0003-3889-6722
  - name: Liam Langley
    orcid: 0000-0001-9754-6517
link-external-newwindow: true #open links in new window
code-link: true #link to packages
code-tools:
      source: https://github.com/AliceTrevail/Code-workshop/blob/9f86b183880d9260c3c7f40007b7732566393d56/R/test.R #location for R script (update if test.R name is changed)
toc: true #table of contents
execute:
  echo: true #show code chunk source (override with "#| output= true/false")
project:
  execute-dir: project #use project as top level
---

## Introduction

This user guide can be used as a walkthrough for reading and processing tracking data files with the `Workflow.R` script. 
You can use the example datasets provided in `TestData`, or try with your own tracking data (see [Pre-flight checks](#pre-flight-checks) for details on data requirements and structure).

The following diagram gives an overview of the workflow (boxes are hyperlinked to each section):

```{mermaid}
%%| fig-width: 9
%%{init:{'flowchart':{'nodeSpacing': 70, 'rankSpacing': 30}}}%%
flowchart LR
  A[Read in data] --> C{Merge}
  B[Read in metadata] --> C
  C --> D(Filter)
  D --> E(Process)
  E --> F(Save for Shiny)
  F --> X(("Shiny app<br/>(optional)")) --> G
  F --->|Skip app|G(Filter II)
  G --> H(Summarise)
  H --> I(Save/export)
  H --> J(Visualise)
    subgraph OPTIONAL ["post-processing (optional)"]
    style OPTIONAL fill:#E4E4E4,stroke:#999,stroke-width:px,stroke-dasharray: 5 5
        direction LR
        Z("- Resample data<br/>- Define segments<br/>- Define trips")
        style Z text-align:left
    end
 H --> OPTIONAL
  click A "#read-in-data-files";
  click B "#merge-with-metadata";
  click C "#merge-with-metadata";
  click D "#filtering-i";
  click E "#processing-i";
  click F "#save-for-shiny";
  click G "#filtering-ii";
  click H "#summarise-data";
  click I "#save-filtered-and-summary-data";
  click J "#visualisation-i";
```

#### Dependencies:

-   This workflow uses the [R programming language](https://www.r-project.org/about.html), run via the [R Studio IDE](https://www.rstudio.com/products/rstudio/)
-   All code embraces the core principles of how to structure ['tidy data'](https://r4ds.had.co.nz/tidy-data.html)
-   We use [RStudio projects](https://r4ds.had.co.nz/workflow-projects.html#rstudio-projects) and the [`here`](https://here.r-lib.org/) package to build relative filepaths that are reproducible
-   Requires [`tidyverse`](https://www.tidyverse.org/), [`data.table`](https://github.com/Rdatatable/data.table), [`sf`](https://r-spatial.github.io/sf/) and [`here`](https://here.r-lib.org/) packages to be installed
-   Use our example data sets in the TestData folder (RFB, GAN, SAP) or provide your own data

#### User inputs

Some code chunks require editing by the user to match the specific dataset being used (particularly if you are using your own data), and are highlighted as below (the ðŸ§  indicates you will need to think about the structure and format of your data when making these edits!):

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

```{r}
#| eval: false
#--------------------#
## USER INPUT START ##
#--------------------#
example_input <- "uservalue" # In the R code, user input sections appear like this
#------------------#
## USER INPUT END ##
#------------------#
```
:::

## 0. Pre-flight checks {#pre-flight-checks}

##### How to use this workflow:
-   We will inspect the data before reading it in, so there is no need to open it in another program (e.g., excel, which can corrupt dates and times)
-   User-defined parameters (see [user inputs](#user-inputs)) are called within the subsequent processing steps
-   Where you see: `## ** Option ** ##`, there is an alternative version of the code to fit some common alternative data formats
-   Throughout, we will use some key functions to inspect the data (e.g., `head` for top rows, `str` for column types, and `names` for column names)

##### Required data structure:
-   Data files should all be stored in a specific folder (ideally within `TestData`)
-   Data for each deployment/individual should be in a separate file
-   ID should be in tracking data file name, and should be the same length for all individuals
-   Tracking data must contain timestamp and at least one other sensor column
-   Metadata file should be in parent directory of data files
-   Metadata should contain one row per individual per deployment

##### The importance of ID:
-   Throughout this workflow, we use ID to refer to the unique code for an individual animal
-   In certain cases, you might have additional ID columns in the metadata (e.g., `DeployID`),
-   or read in data with a unique TagID instead of ID.
-   This code will work as long as all of the relevant info is included in the metadata
-   For more info and helpful code, see the FAQ document & troubleshooting script

##### How to troubleshoot problems if something doesn't work with your data:
-   Refer to the FAQ document in the github page
-   This signposts to helpful resources online (e.g., spatial co-ordinate systems)
-   See the troubleshooting code scripts that we've written to accompany this workflow
-   e.g., using multiple ID columns for re-deployments of tags/individuals

##### Load required libraries

Just before starting we load in all the packages we will need for the workflow (also referenced in the [Dependencies](#dependencies) section).

```{r}
#| output: false
library(tidyverse) #installed using install.packages("tidyverse")
library(lubridate) #installed with tidyverse but needs loading manually
library(sf)
library(data.table)
library(here)
```


## 1. Read in data files {#read-in-data-files}

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

Set filepath for the folder containing raw data files (this code will try to open all files matching the file pattern within this folder, so it is best if this folder only contains the raw data files).

```{r}
filepath <- here("TestData", "RFB") #create relative filepath using folder names
```

Define common file pattern to look for. An asterisk (`*`) is the wildcard, will will match any character except a forward-slash (e.g. `*.csv` will import all files that end with ".csv").

```{r}
filepattern <- "*.csv" # data file format (use "*.csv" to import all csv files within filepath folders)
```

Let's view the file names, to check that we have the files we want & find ID position (this list will include names of sub-folders).

```{r}
ls_filenames <- list.files(path = filepath, recursive = TRUE, pattern = filepattern)
ls_filenames
```

Adjust these numbers for extracting the ID number from file name using [`stringr`](https://stringr.tidyverse.org/index.html) (e.g. to extract GV37501 from "GV37501_201606_DG_RFB.csv", we want characters 1-7). </br>
**NB:** this approach only works if ID's are in the same position --- see the [`str_sub`](https://stringr.tidyverse.org/reference/str_sub.html) documentation for other options.

```{r}
IDstart <- 1 #start position of the ID in the filename 
IDend <- 7 #end position of the ID in the filename
```

Now, let's inspect the data by reading in the top of the first data file as raw text. To inspect the first row of all data files (if you wanted to check column names), you can remove the `[1]` and change `n_max = 1`).

```{r}
test <- fs::dir_ls(path = filepath, recurse = TRUE, type = "file",  glob = filepattern)[1]
read_lines(test, n_max = 5)  # change n_max to change the number of rows to read in
```

Define number of lines at top of file to skip (e.g. if importing a text file with additional info at top).
```{r}
skiplines <- 0
```

Define date format(s) used (for passing to [`lubridate`](https://lubridate.tidyverse.org/reference/lubridate-package.html)) (`d` = day as decimal, `m` = month as decimal, `y` = year without century - 2 digits, `Y` = year with century - 4 digits).
`Lubridate` can even parse more than one date/time format within a dataframe, so if your data include multiple formats, make sure they are all included. Here, we've included some common combinations:

```{r}
date_formats <- c("dmY", "Ymd") #specify date formats (e.g. "dmY" works for 01-12-2022 and 01/12/2022)
datetime_formats <- c("Ymd HMS") #format used for date and time ("HMS" for hour, minutes and seconds)
```

Define time zone for tracking data.

```{r}
trackingdatatimezone <- "GMT"
```

By default, the below code will find column names from the first row of data. If you want to specify your own column names, do so here as a character vector, or use `set colnames <- FALSE` to automatically number columns.

```{r}
colnames <- TRUE
```

Here, we use the function `read_delim` and specify the delimiter to make this code more universal (you can find extra information on this in the [`readr` documentation](https://readr.tidyverse.org/reference/read_delim.html)).

Some delimiter examples:

-   `","` = comma delimited (equivalent to using `read_csv` -- saved as extension `.csv`)
-   `"\t"` = tab delimited (equivalent to using `read_tsv` --- saved as extension `.tsv`)
-   `" "` = whitespace delimited (equivalent to using `read_table`)

Let's inspect the data again, this time skipping rows if set, to check the file delimiter.

```{r}
read_lines(test, n_max = 5, skip = skiplines)
```

Set delimiter to use within `read_delim`.

```{r}
user_delim <- ","
user_trim_ws <- TRUE # Should leading/trailing whitespaces be trimmed from each field before parsing?
```

:::

#### Read in and merge all tracking data files {.tabset}

::: panel-tabset
##### tidyr

```{r}
df_combined <- fs::dir_ls(path = filepath, #use our defined filepath
                          glob = filepattern, #use our defined filepattern
                          type = "file",  #only search for files
                          recurse = TRUE) %>% #search all sub-folders
  purrr::set_names(nm = basename(.)) %>% #removing path prefix
  purrr::map_dfr(read_delim, .id="filename", #read all files using filename as ID column
                 col_types = cols(.default = "c"), #read all columns as characters
                 col_names = colnames, #use our defined colnames
                 skip = skiplines, #user defined skiplines
                 delim = user_delim, 
                 trim_ws = user_trim_ws) %>%  
  mutate(ID = str_sub(string = filename, #extract ID from filename
                      start = IDstart, end = IDend), #user-defined string positions
         .after = filename) #reposition the new ID column
df_combined
```

##### tidyr with IDs (option)

If your data are combined into one or multiple csv files already containing an ID column, use the following code
(this is the same as the normal tidyr pipe, but doesn't create a new ID column from the file name):

```{r}
#| eval: false
## ** Option ** ##
df_combined <- fs::dir_ls(path = filepath, glob = filepattern, #use our defined filepath and pattern
                          type = "file", recurse = TRUE) %>% # recurse = T searches all sub-folders
   purrr::map_dfr(read_delim, col_types = cols(.default = "c"), col_names = colnames, 
                  skip = skiplines, delim = user_delim, trim_ws = user_trim_ws) 
df_combined
```

##### baseR (option)

This is how you would read in data with base R code.

```{r}
#| eval: false
raw_filenames <- list.files(path = filepath, recursive=TRUE, pattern=".csv", full.names = T)
ls_raw_files <-lapply(raw_filenames, function(x){fread(x)})
names(ls_raw_files) <- raw_filenames
head(ls_raw_files)

df_combined <- rbindlist(ls_raw_files, idcol = "filepath", fill = T) #rbind with data table and save ID column 

df_combined$ID <- str_sub(df_combined$filepath, start=IDstart, end=IDend) #extract ID string from filename
```
:::

#### Slim down dataset {.tabset}
::: panel-tabset

##### Select normal columns

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

Here we select necessary columns & coerce column names

-   Compulsory columns = ID, Date & Time (or single DateTime column?)
-   Optional columns depending on sensor type, e.g. Lat, Lon, error

```{r}
df_slim <- data.frame(ID = df_combined$ID,
                      Date = df_combined$Date,
                      Time = df_combined$Time,
                      Lat = df_combined$Latitude,
                      Lon = df_combined$Longitude)
head(df_slim)
```
:::

##### Select custom columns (option)

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

Here's an example of how to change the above code for data with different columns and column names. This code works with immersion data recorded by a GLS logger (no location data)

```{r}
#| eval: false
df_slim <- data.frame(ID = df_combined$ID,
                      Date = df_combined$`DD/MM/YYYY`,
                      Time = df_combined$`HH:MM:SS`,
                      Immersion = df_combined$`wets0-20`)
```
:::

:::

#### Dates and Times {.tabset}

Now our `df_slim` is ready, we need to create a `DateTime` column. We can either parse `Date` and `Time` columns separately and combine, or parse a single `DateTime` column (if already present):

::: panel-tabset

##### Date + Time

If your date and time are in separate columns, create `Datetime` by appending `Time` to `Date` object using `parse_date_time`)

```{r}
df_slim$Date <- lubridate::parse_date_time(df_slim$Date, #parse date column
                                           orders = date_formats) 
df_slim$DateTime <- lubridate::parse_date_time(paste(df_slim$Date, df_slim$Time), #create DateTime object
                                               orders = datetime_formats, 
                                               tz = trackingdatatimezone) 
```

::: {.callout-note icon="true"}
`failed to parse` warnings mean a date or time has not formatted (dealt with later)
::: 

##### DateTime (option)

If date and time are already combined in one column (named `DateTime`), parse to ensure correct format & time zone

```{r}
#| eval: false
## ** Option ** ##
df_slim$DateTime <- lubridate::parse_date_time(df_slim$DateTime, 
                                               orders = datetime_formats, 
                                               tz = trackingdatatimezone)
```

::: {.callout-note icon="true"}
`failed to parse` warnings mean a date or time has not formatted (dealt with later)
:::

:::

Lastly, we make a `df_raw` dataframe by sorting using ID and DateTime, dropping NA's in DateTime column

```{r}
df_raw <- df_slim %>% 
  arrange(ID, DateTime) %>%
  drop_na(DateTime) #remove NA's in datetime column
head(df_raw)
```

We can clean up intermediate files/objects by listing everything we want to keep (i.e. remove everything else)

```{r}
#| output: false
rm(list=ls()[!ls() %in% c("df_raw", 
                          "date_formats","datetime_formats","trackingdatatimezone")]) 
```

## 2. Merge with metadata {#merge-with-metadata}

Metadata are an essential piece of information for any tracking study, as they contain important information about each of your data files, such as tag ID, animal ID, or deployment information, that we can add back into to our raw data when needed. For example, this is what the first few columns of a metadata file looks like for our example Red Footed Booby data:

|TagID | BirdID | DeployID | Species | Colony | Age |	BreedingStage	| DeploymentDate |
|:----:|:-------:|:--------:|:------:|:------:|:---:|---------------|----------------|
| 5    | GV37501 | 1        | RFB	   | DG     |Adult| Chick rearing | 25/06/2016     |
| 46   | GV37503 | 1	      | RFB    | DG     |Adult| Chick rearing | 26/06/2016     |
| 5    | GV37734 | 1        | RFB	   | NI     |Adult| Chick rearing | 08/07/2018     |

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

First we define the path to our metadata file:

```{r}
filepath_meta <- here("Testdata","RFBTest_Metadata.csv")
```

Then much like in Step 1, we define the date format(s) used (for passing to [`lubridate`](https://lubridate.tidyverse.org/reference/lubridate-package.html)) (`d` = day as decimal, `m` = month as decimal, `y` = year without century - 2 digits, `Y` = year with century - 4 digits).
Here, we've included common combinations, which you'll need to modify if your metadata include a different format.

```{r}
metadate_formats <- c("dmY", "Ymd") #specify date format used in metadata
metadatetime_formats <- c("dmY HMS", "Ymd HMS") #specify date & time format
metadatatimezone <- "Indian/Chagos" #specify timezone used for metadata
```

Next we read in the metadata file (make sure to check data format matches the `read_` function used).

```{r}
#| output: false
df_metadata <- read_csv(filepath_meta) #read in the metadata file
names(df_metadata)
```

Then we select necessary comments & coerce column names, making sure to provide four compulsory columns: **ID**, **deployment date** & **deployment time**. We can also provide optional columns depending on sensor type: e.g. colony, sex, age, central place (CP) Lat, CP Lon. Depending on whether you know the CP for each individual, you can use one of the two following approaches, and delete columns where appropriate.

::: panel-tabset

##### Select metadata columns

If you have multiple ID columns like TagID/DeployID, include them here (for example, if one individual was tracked over multiple deployments/years, or if one tag was re-deployed on multiple individuals). For more information and helpful code, see the [FAQ document](FAQ's.html) and [troubleshooting script](https://github.com/AliceTrevail/Code-workshop/blob/c815d760947d7f608ff9aeeaa902f3aa878b7cdc/R/Troubleshoot%20-%20multiple%20ID%20columns.R).

```{r}
df_metadataslim <- data.frame(ID = df_metadata$BirdID,
                              DeployID = df_metadata$DeployID,
                              DeployDate_local = df_metadata$DeploymentDate, # note "local" in colname
                              DeployTime_local = df_metadata$DeploymentTime, # note "local" in colname
                              RetrieveDate_local = df_metadata$RetrievalDate, # note "local" in colname
                              RetrieveTime_local = df_metadata$RetrievalTime, # note "local" in colname
                              CPLat = df_metadata$NestLat,
                              CPLon = df_metadata$NestLong,
                              Species = df_metadata$Species,
                              Population = df_metadata$Colony,
                              Age = df_metadata$Age, 
                              BreedingStage = df_metadata$BreedingStage)
```

##### Individual CP unknown (option)

If central place for each individual is not known, add population-level central places here by first creating a custom dataframe of population CPs:
```{r}
#| eval: false
## ** Option ** ##
df_PopCPs <- tribble(
  ~Population,  ~CPLat,    ~CPLon,
  "DG",         -7.24,     72.43,
  "NI",         -5.68,     71.24
  )
```

Then we merge `df_metadataslim` and `df_PopCPs.` The `Population` column  must be present in both dataframes (this will overwrite `CPLat` and `CPLon` if present in `df_metadataslim`).
```{r}
#| eval: false
df_metadataslim  %<>%
  select(matches(setdiff(names(df_metadataslim), c("CPLat", "CPLon")))) %>%
  left_join(., df_PopCPs, by = "Population")
```

:::

:::

Format all dates and times, combine them and specify timezone (tz)

```{r}
df_metadataslim <- df_metadataslim %>%
  mutate(across(contains('Date'), 
                ~parse_date_time(., orders=metadate_formats))) %>% #format all Date column with lubridate
  mutate(Deploydatetime = parse_date_time(paste(DeployDate_local, DeployTime_local),#create deploy datetime
                                          order=metadatetime_formats, 
                                          tz=metadatatimezone),
         Retrievedatetime = parse_date_time(paste(RetrieveDate_local, RetrieveTime_local), #create retrieve datetime
                                            order=metadatetime_formats, 
                                            tz=metadatatimezone)) %>% 
  select(-c(DeployDate_local,DeployTime_local,RetrieveDate_local,RetrieveTime_local)) %>%
  mutate(across(contains('datetime'), #return datetime as it would appear in a different tz
                ~with_tz(., tzone=trackingdatatimezone))) 
```

Merge metadata with raw data using ID column

```{r}
df_metamerged <- df_raw %>%
  left_join(., df_metadataslim, by="ID") #add metadata to summarised data (nest lat/long)
```

Remove intermediate files/objects by specifying objects to keep.

```{r}
#| output: false
rm(list=ls()[!ls() %in% c("df_metamerged")]) #specify objects to keep
```

## 3. Filtering I {#filtering-i}

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

Define your own no/empty/erroneous data values in `Lat` and `Lon` columns (e.g. "bad" values specified by the tag manufacturer).

```{r}
No_data_vals <- c(0, -999)
```

Define a vector of columns which can't have NAs

```{r}
na_cols <- c("Lat", "Lon", "DateTime", "ID") #column to check for na's
```

:::

Now we pipe the data through a series of functions to drop NAs in specified columns, filter out user-defined `no_data_values` in `Lat` `Lon` columns, remove duplicates, remove undeployed locations and filter out locations within temporal cut-off following deployment.

```{r}
df_clean <- df_metamerged %>%
              drop_na(all_of(na_cols)) %>% 
              filter(!Lat %in% No_data_vals & !Lon %in% No_data_vals) %>% # filter bad data values in Lat Lon columns
              distinct(DateTime, ID, .keep_all = TRUE) %>% # if you have ACC data this might cause issues
              filter(Deploydatetime < DateTime & DateTime < Retrievedatetime) # filter to deployment period, only
head(df_clean)
```

Finally we remove intermediate files/objects

```{r}
rm(list=ls()[!ls() %in% c("df_clean")]) #specify objects to keep
```

## 4. Processing I {#processing-i}

#### Some useful temporal and spatial calculations on the data

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

First we need to specify the co-ordinate projection systems for the tracking data and meta data. The default here is lon/lat for both tracking data & metadata, for which the EPSG code is 4326. For more information see the [CRS section of the FAQ's](FAQ's.html#what-is-a-crs).

```{r}
tracking_crs <- 4326 #Only change if data are in a different coordinate system
meta_crs <- 4326 #Only change if data are in a different coordinate system
```
:::

Next we transform coordinates of data, and perform spatial calculations. This requires spatial analysis, and so it is good practice to run all spatial analyses in a coordinate reference system that uses metres as a unit.

As an example, we will use Spherical Mercator projection â€” *aka* "WGS" (crs = 3857). It's important to consider the location and scale of your data (e.g., equatorial/polar/local scale/global scale) when choosing a projection system.

Depending on whether you know the central place (CP) for each individual, you can use one of the two following approaches:

::: panel-tabset

##### With CP data

This is how we calculate bearings relative to CP:
```{r}
df_diagnostic <-  df_clean %>%
  ungroup() %>% #need to ungroup to extract geometry of the whole dataset
  mutate(geometry_GPS = st_transform( #assign geometry and transform to WGS for dist calcs
            st_as_sf(., coords=c("Lon","Lat"), crs=tracking_crs), crs = 3857)$geometry,
         geometry_CP = st_transform( #assign geometry and transform to WGS for dist calcs
            st_as_sf(., coords=c("CPLon","CPLat"), crs=meta_crs), crs = 3857)$geometry) %>% 
  group_by(ID) %>% #back to grouping by ID for calculations per individual
  mutate(dist = st_distance(geometry_GPS, lag(geometry_GPS), by_element = T), #distance travelled from previous fix
         difftime = difftime(DateTime, lag(DateTime), units="secs"), #time passed since previous fix
         netdisp = st_distance(geometry_GPS, geometry_GPS[1], by_element = T), #calculate distance to first location
         speed = as.numeric(dist)/as.numeric(difftime), #calculate speed (distance/time)
         dLon = as.numeric(Lon)-lag(as.numeric(Lon)), #difference in longitude, relative to previous location
         dLat = as.numeric(Lat)-lag(as.numeric(Lat)), #difference in longitude, relative to previous location
         turnangle = atan2(dLon, dLat)*180/pi + (dLon < 0)*360, #angle (in degrees) from previous to current location
         CPdist = st_distance(geometry_GPS, geometry_CP, by_element = T), #calculate distance to CP
         dLon_CP = as.numeric(Lon)-CPLon, #difference in longitude to CP
         dLat_CP = as.numeric(Lat)-CPLat, #difference in longitude to CP
         CPbearing = atan2(dLon_CP, dLat_CP)*180/pi + (dLon_CP < 0)*360) %>% #bearing (in degrees) from CP
  ungroup() %>% select(-c(geometry_GPS, geometry_CP, dLon, dLat, dLon_CP, dLat_CP)) #ungroup and remove geometries
```

##### Without CP data

If your data do not include a Central Place, you can use the below code to calculate bearings relative to first location instead:

```{r}
#| eval: false
## ** Option ** ##
df_diagnostic <-  df_clean %>%
  ungroup() %>% #need to ungroup to extract geometry of the whole dataset
  mutate(geometry_GPS = st_transform( # assign geometry and transform to WGS for dist calcs
            st_as_sf(., coords=c("Lon","Lat"), crs=tracking_crs), crs = 3857)$geometry) %>% 
  group_by(ID) %>% #back to grouping by ID for calculations per individual
  mutate(dist = st_distance(geometry_GPS, lag(geometry_GPS), by_element = T), #distance travelled from previous fix
         difftime = difftime(DateTime, lag(DateTime), units="secs"), #time passed since previous fix
         netdisp = st_distance(geometry_GPS, geometry_GPS[1], by_element = T), #calculate distance to first location
         speed = as.numeric(dist)/as.numeric(difftime), #calculate speed (distance/time)
         dLon = as.numeric(Lon)-lag(as.numeric(Lon)), #difference in longitude, relative to previous location
         dLat = as.numeric(Lat)-lag(as.numeric(Lat)), #difference in longitude, relative to previous location
         turnangle = atan2(dLon, dLat)*180/pi + (dLon < 0)*360, #angle from previous to current location
         dLon_net = as.numeric(Lon)-as.numeric(Lon)[1], #difference in longitude between current and first location
         dLat_net = as.numeric(Lat)-as.numeric(Lat)[1], #difference in longitude between current and first location
         netbearing = atan2(dLon_net, dLat_net)*180/pi + (dLon_net < 0)*360) %>% #bearing from first to current location
   ungroup() %>% select(-c(geometry_GPS, dLon, dLat, dLon_net, dLat_net)) #ungroup and remove geometries
```

:::

## 5. Save for Shiny

Here we're going to save `df_diagnostic` to use in the shiny app provided. The app is designed to explore how further filtering and processing steps affect the data.

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

First we'll add species code, as it's good practice to include species in the data frame & file name
```{r}
species_code <- "RFB"
```

Then we define a file path for saving the working dataframe files (if you're defining a new folder, make sure to create it before attempting to save into it).
```{r}
filepath_dfout <- here("TestDataOutputs","WorkingDataFrames")
```

Next we define file name for the saved file by pasting the species code to `_diagnostic` (can change if you want to use a different naming system).
```{r}
filename_dfout <- paste0(species_code, "_diagnostic")
```

If not added from the metadata, add a species column and any other columns here relevant to your data *(optional)*
```{r}
#| eval: false
## ** Option ** ##
df_diagnostic$Species <- species_code
```

:::

Finally we save the `df_diagnostic` as a csv file using the variables created above.

```{r}
#| eval: false
write_csv(df_diagnostic, file = here(filepath_dfout, paste0(filename_dfout,".csv")))
```

Remove everything except `df_diagnostic` ahead of the next step.

```{r}
rm(list=ls()[!ls() %in% c("df_diagnostic")]) #specify objects to keep
```


## 6. Filtering II

This second filtering stage is designed to remove outliers in the data, and you can use outputs from the shiny app to inform these choices. If you don't need to filter for outliers, skip this step and keep using `df_diagnostic` in the next steps.

##### Define threshold values

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

First we define a period to filter after tag deployment, when all points before the cutoff will be removed (e.g. to remove potentially unnatural behaviour following the tagging event). We define this period using the `as.period` function, by providing an integer value and time unit (e.g. hours/days/years). This code below specifies a period of 30 minutes:

``` {r}
filter_cutoff <- as.period(30, unit="minutes") 
```

Then we define speed threshold in m/s, which we will use to remove any points with faster speeds.
``` {r}
filter_speed <- 20
```

Next we define a net displacement (distance from first point) threshold and specify units. Any points further away from the first tracking point will be removed:
``` {r}
filter_netdisp_dist <- 200
filter_netdist_units <- "km" # e.g., "m", "km"
```

:::

##### Implement filters

Create net displacement filter using distance and units
``` {r}
filter_netdisp <- units::as_units(filter_netdisp_dist, filter_netdist_units)
```

Filter df_diagnostic
``` {r}
df_filtered <- df_diagnostic %>%
  filter(Deploydatetime + filter_cutoff < DateTime, # keep times after cutoff
         speed < filter_speed, # keep speeds slower than speed filter
         netdisp < filter_netdisp) # keep distances less than net displacement filter
head(df_filtered)
```

Remove intermediate files/objects
``` {r}
rm(list=ls()[!ls() %in% c("df_filtered")]) #specify objects to keep
```


## 7. Summarise data

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

Set the units to display sampling rate in the summary table

```{r}
sampleRateUnits <- "mins" 
```

##### Define levels of grouping factors to summarise over

Firstly, down to population level. Here, we are working on data from one population & year, and so use `Species` and `Population.` Add any other relevant grouping factors here (e.g. Country / Year / Season / Age).
```{r}
grouping_factors_poplevel <- c("Species", "Population") 
```

Secondly, down to individual level (add `DeployID` for example if relevant).
```{r}
grouping_factors_indlevel <- c("ID")
```

:::

##### Create summary tables

Create a small function to calculate standard error.
```{r}
se <- function(x) sqrt(var(x, na.rm = T) / length(x[!is.na(x)]))
```

Create a summary table of individual-level summary statistics:
```{r}
df_summary_ind <- df_filtered %>%
  group_by(across(c(grouping_factors_poplevel, grouping_factors_indlevel))) %>%
  summarise(NoPoints = NROW(ID), # number of fixes
            NoUniqueDates = length(unique(Date)), # number of tracking dates
            FirstDate = as.Date(min(Date)), # first tracking date
            LastDate = as.Date(max(Date)), # last tracking date
            SampleRate = mean(as.numeric(difftime, units = sampleRateUnits), na.rm = T), # sample rate mean
            SampleRate_se = se(as.numeric(difftime, units = sampleRateUnits))) # sample rate standard error
df_summary_ind
```

Create a table of population-level summary statistics:
```{r}
df_summary_pop <- df_summary_ind %>% # use the individual-level summary data
  group_by(across(grouping_factors_poplevel)) %>%
  summarise(NoInds = length(unique(ID)), # number of unique individuals
            NoPoints_total = sum(NoPoints), # total number of tracking locations
            FirstDate = as.Date(min(FirstDate)), # first tracking date
            LastDate = as.Date(max(LastDate)), # last tracking date
            PointsPerBird = mean(NoPoints), # number of locations per individual: mean
            PointsPerBird_se = se(NoPoints), # number of locations per individual: standard error
            DatesPerBird = mean(NoUniqueDates), # number of tracking days per bird: mean
            DatesPerBird_se = se(NoUniqueDates), # number of tracking days per bird: standard error
            SampleRate_mean = mean(SampleRate), # sample rate mean
            SampleRate_se = se(SampleRate)) # sample rate standard error
df_summary_pop
```


Remove intermediate files/objects by specifying which objects to keep:
```{r}
rm(list=ls()[!ls() %in% c("df_filtered", "df_summary_ind", "df_summary_pop")])
```


## 8. Save filtered and summary data

::: {.callout-warning icon="false"}
#### ðŸ§  User input required

First we define a species code:
```{r}
species_code <- "RFB"
```

Next we define the folder file path for saving our filtered data
```{r}
filepath_filtered_out <- here("TestDataOutputs","WorkingDataFrames")
```

Then we define the file path for saving summary dataframes
```{r}
filepath_summary_out <- here("TestDataOutputs","SummaryDataFrames")
```

Here we define file names for saved files, and paste the species code to `_summary_`, followed by `ind` (individual level) or `pop` (population level). You can change this if you want to use a different naming system.
```{r}
filename_filtered_out <- paste0(species_code, "_filtered")
filename_summary_ind_out <- paste0(species_code, "_summary_ind")
filename_summary_pop_out <- paste0(species_code, "_summary_pop")
```

:::

Now we can save all our dataframes as .csv files using our defined values
```{r}
write_csv(df_filtered, file = here(filepath_filtered_out, paste0(filename_filtered_out,".csv")))
write_csv(df_summary_ind, file = here(filepath_summary_out, paste0(filename_summary_ind_out,".csv")))
write_csv(df_summary_pop, file = here(filepath_summary_out, paste0(filename_summary_pop_out,".csv")))
```

Lastly we remove intermediate files/objects
```{r}
rm(list=ls()[!ls() %in% c("df_filtered", "df_summary_ind", "df_summary_pop")]) #specify objects to keep
```


## 9. Visualisation I

::: {.callout-warning icon="false"}
#### User input required

Define parameters for reading out plots, and define device to read plots out as e.g. tiff/jpeg

```{r}
device <- "tiff"
```

Define units for plot size - usually mm

```{r}
units <- "mm"
```

Define plot resolution in dpi - 300 usually minimum

```{r}
dpi <- 300
```

Define filepath to read out plots

```{r}
out_path <- here("TestDataOutputs","Figures")
```

Define species code for figure names
```{r}
species_code <- "RFB"
```

We plot maps over a topography base-layer which can include terrestrial (elevation) and marine (bathymetry/water depth) data. To set legend label for topography data, relevant to your data.
```{r}
topo_label = "Depth (m)"
```

:::

Load additional libraries for spatial visualisation (optional step - could be removed/improved later!!)

::: {.callout-note icon="true"}
#### If you see a `masking` warning these are fine. Watch out for packages that aren't installed yet
```{r}
#| output: false
library(rnaturalearth)
library(marmap)
library(plotly)
```
:::

Version of data for plotting which includes transforming required columns to numeric and creating time elapsed columns

```{r}
df_plotting <- df_filtered %>%
  group_by(ID) %>%
  mutate(diffsecs = as.numeric(difftime),
         secs_elapsed = cumsum(replace_na(diffsecs, 0)),
         time_elapsed = as.duration(secs_elapsed),
         days_elapsed = as.numeric(time_elapsed, "days")) %>%
  mutate(across(c(dist,speed,CPdist, Lat, Lon), as.numeric))
```

Create a map of all points. Set the plot limits as the max and min lat/longs as the tracking data

First set up a basemap to plot over:
-   Use rnatural earth low res countries basemap
-   co-ordinates in lat/long to match other spatial data

```{r}
countries <- ne_countries(scale = "medium", returnclass = "sf")
```

Define min and max co-ordinates based on extent of tracking data, for adding bathymetry extracted from NOAA database. 

```{r}
minlon <- min(df_plotting$Lon)
maxlon <- max(df_plotting$Lon)

minlat <- min(df_plotting$Lat)
maxlat <- max(df_plotting$Lat)
```

Load in bathymetry basemap. Set limits slightly beyond tracking data to make a buffer so no gaps when plotting

```{r}
#| output: false
base_topography_map <- getNOAA.bathy(
  lon1 = minlon - 0.1, lon2 = maxlon + 0.1,
  lat1 = minlat - 0.1, lat2 = maxlat + 0.1, 
  resolution = 1)
```

Fortify bathymetry data for plotting

```{r}
base_topography_fort = fortify(base_topography_map)
```

Create base map with correct extent, topography, country outlines, etc.,

```{r}
map_base <- ggplot() + 
  geom_raster(data = base_topography_fort, 
              aes(x=x, y=y, fill=z), alpha = 0.9) +
  # add colour scheme for the fill
  scale_fill_viridis_c(option="mako", name = topo_label) + 
  # add map of countries over the top
  geom_sf(data = countries, aes(geometry = geometry), fill = NA) + 
  # add central place points
  geom_point(data = df_plotting, 
             aes(x = CPLon, y = CPLat),
             colour = "#FF3300", 
             fill ="#FF3300", 
             shape = 23, 
             size = 2) +
  # set plot limits
  coord_sf(xlim = c(minlon, maxlon), 
           ylim = c(minlat, maxlat), crs = 4326) +
  # add labels
  labs(x = "Longitude", y = "Latitude") +
  theme(axis.text=element_text(colour="black"),
        axis.title.x = element_text(size = 15),
        axis.text.x = element_text(hjust=0.7),
        axis.title.y = element_text(angle=90, vjust = 0.4, size = 15),
        axis.text.y = element_text(hjust=0.7, angle=90, vjust=0.3)) +
  theme_light()
map_base
```

::: panel-tabset

##### Population

Plot a combined map of all tracking locations:

```{r}
map_alllocs <- map_base + 
  geom_point(data = df_plotting, # add GPS points
             aes(x = Lon, y = Lat), 
             alpha = 0.8, size = 0.5) 
map_alllocs
```

##### Several individuals

Plot a map of individual locations, colouring points by speed, and faceting by ID

```{r}
map_individuals <- map_base + 
  geom_point(data = df_plotting,  # add GPS points and paths between them
             aes(x = Lon, y = Lat, col = speed), 
             alpha = 0.8, size = 0.5 ) +
  geom_path(data = df_plotting, aes(x = Lon, y = Lat, col = speed), 
            alpha = 0.8, size = 0.5 ) +
  # colour birds using scale_colour_gradient2
  scale_colour_gradient2(name = "Speed", low = "blue", mid = "white", high = "red", 
                         midpoint = (max(df_plotting$speed,na.rm=TRUE)/2)) +
  ##facet for individual
  facet_wrap(~ ID, ncol = n_distinct(df_plotting$ID))
map_individuals
```

##### Many individuals

In previous plots, we've split the population into individual facets. This works fine on the example code, where we only have a few individuals, but if you have more individuals and the facets are too small, you can split the plot onto multiple pages. Use the below code to use `facet_wrap_paginate` from the `ggforce` package:

```{r}
#| messages: false
## ** Option ** ##
library(ggforce)
## save plot as object to later extract number of pages (e.g. 2 per page):
map_individuals <- map_base +
   geom_point(data = df_plotting, # add GPS points and paths between them
              aes(x = Lon, y = Lat, col = speed), 
              alpha = 0.8, size = 0.5 ) +
   geom_path(data = df_plotting, 
             aes(x = Lon, y = Lat, col = speed), 
             alpha = 0.8, size = 0.5 ) +
   scale_colour_gradient2(name = "Speed", # colour birds using scale_colour_gradient2
                          low = "blue", 
                          mid = "white", 
                          high = "red", 
                          midpoint = (max(df_plotting$speed,na.rm=TRUE)/2)) +
   facet_wrap_paginate(~ID, ncol = 2, nrow= 1, page = 1) #facet for individual
```

How many pages of plots?
```{r}
 n_pages(map_individuals)
```
 
Run through different values of page to show each page in turn
```{r}
map_individuals
```

:::

Save maps for further use using `ggsave` function.

```{r}
#| eval: false
ggsave(plot = map_alllocs, 
       filename = paste0(species_code, "_map_all_locs.tiff"),
       device = device,
       path = out_path, 
       units = units, width = 200, height = 175, dpi = dpi,   
)

ggsave(plot = map_individuals, 
       filename = paste0(species_code, "_map_individuals.tiff"),
       device = device,
       path = out_path, 
       units = units, width = 200, height = 175, dpi = dpi,   
)
```

Create a time series plot of speed, faceted for each individual. 

```{r}
speed_time_plot <- df_plotting %>% #speed over time
  ggplot(data=., aes(x=days_elapsed, y=speed, group=ID)) +
  geom_line() + #coord_cartesian(ylim=c(0,25)) +
  xlab("time elapsed (days)") + ylab("speed (m/s)") +
  facet_wrap(~ID, nrow= n_distinct(df_plotting$ID)) +
  theme(axis.text=element_text(colour="black")) +
  theme_light()
speed_time_plot
```

::: {.callout-note icon="true"}

Warnings about `non-finite` values for speed/step length plots are expected and usually refer to the first location for each individual (i.e. number of non-finite values should be equal to number of individuals)

:::

Save plot for further use

```{r}
#| eval: false
ggsave(plot = speed_time_plot, 
       filename = paste0(species_code, "_speed_timeseries_plot.tiff"),
       device = device,
       path = out_path, 
       units = units, width = 200, height = 175, dpi = dpi)
```

Create a histogram of point to point speeds (can adjust binwidth and x limits manually)

```{r}
speed_hist <- df_plotting %>% #speed histogram
  ggplot(data=., aes(speed)) +
  geom_histogram(binwidth=0.1, alpha=0.7) +
  geom_density(aes(y =0.1*..count..)) +
  #xlim(-0.1,5) +
  xlab("speed (m/s)") + ylab("count") +
  facet_wrap(~ID, nrow= n_distinct(df_plotting$ID)) +
  theme(axis.text=element_text(colour="black"))+
  theme_light()
speed_hist
```

Save plot for further use

```{r}
#| eval: false
ggsave(plot = speed_hist, 
       filename = paste0(species_code, "_speed_histogram.tiff"),
       device = device,
       path = out_path,
       units = units, width = 200, height = 175, dpi = dpi,   
)
```

Create a time series plot of step lengths (faceted for each individual)

```{r}
step_time_plot <- df_plotting %>% #step length over time
  ggplot(data=., aes(x=days_elapsed, y=as.numeric(netdisp), group=ID)) +
  geom_line() +
  xlab("time elapsed (days)") + ylab("Distance from first fix (m)") +
  facet_wrap(~ID, nrow= n_distinct(df_plotting$ID)) +
  theme(axis.text=element_text(colour="black"))+
  theme_light()
step_time_plot
```

Save plot for further use

```{r}
#| eval: false
ggsave(plot = step_time_plot, 
       filename = paste0(species_code, "_step_time_plot.tiff"),
       device = device,
       path = out_path,
       units = units, width = 200, height = 175, dpi = dpi)
```

Create a histogram of step lengths (can adjust binwidth and x limits manually)

```{r}
#| eval: false
step_hist <- df_plotting %>% #step histogram
  ggplot(data=., aes(as.numeric(dist))) +
  geom_histogram(binwidth=1, alpha=0.7) +
  geom_density(aes(y =1*..count..)) +
  ##xlim(0,500) +
  xlab("step length (m)") + ylab("count") +
  facet_wrap(~ID, nrow= n_distinct(df_plotting$ID))+
  theme_light()
```

Save plot for further use

```{r eval=FALSE}
ggsave(plot = step_hist, 
       filename = paste0(species_code, "_step_hist.tiff"),
       device = device,
       path = out_path,
       units = units, width = 200, height = 175, dpi = dpi)
```


```{r}
#| eval: false
#| child: child_doc.qmd
```
