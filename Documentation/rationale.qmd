---
title: "Manuscript Outline"
format: html
toc:true
author:
  - name: Alice Trevail
    orcid: 0000-0002-6459-5213
  - name: Stephen Lang
    orcid: 0000-0001-5820-4346
  - name: Luke Ozsanlav-Harris
    orcid: 0000-0003-3889-6722
  - name: Liam Langley
    orcid: 0000-0001-9754-6517
link-external-newwindow: true #open links in new window
---

# Outline

### Title: Reproducible walkthrough for cleaning, processing and visualising animal tracking data (in R)

### Keywords
Movement, biologging, processing, reproducibility, tracking data, cleaning, shiny app, tidyverse, sf, R

## 1.0 Introduction

-   We provide a guide to start to analyse tracking data

### 1.1 Identifying the problem we want to address

-   What leads the narrative? Learning tool or Reproducibility?
-   Learning how to analyse data is hard, there is a steep learning curve
-   The landscape has shifted quite a lot
-   The data cleaning is a really important step that can have downstream consequences
-   This is a universal step for everyone working with tracking data
-   Everyone from undergrads to PI to using GPS data that needs cleaning. Often they don't know where to look for guidance
-   Lots of time wasted in people re-doing the same cleaning steps on their own
-   If you are not doing these cleaning steps then you should probably worry :(

### 1.2 Identifying what has already been done

-   Roocio highlighted all of the R packages but most of these already required clean data
-   Why our approach is different from the high throughput paper (Too many functions that users find hard to de-bug)
-   Moveapps- magnifies the problems of the high throughput paper, don't know what is even happening

### 1.3 Why our guide is important and different from other papers

-   Really sketch out our niche compared to other papers
-   We are the first step in any analysis approach, the base of the tree
-   This work for any location data really... GPS, GLS and Argos
-   This is also a flexible learning tool. A pipeline that can be adapted
-   This is an open and reproducible way of working
-   Don't rely on lots of packages, just rely on tidyverse, sf and data.table (core packages, maintained)
-   Funnels people from the very start to a point when they can start their bespoke analysis


### 1.4 What are the aims!

-   A common starting point
-   Flexible and readable chunks of code that do what they say on the tin
-   Prioritising code readability over speed/efficiency
-   Clear and concise overview of how to go from data to analyses
-   Pipeline that can be adapted for own use cases
-   Provide both the code and the underlying explanation for what the code does
-   All while being reliant on functions from only a few core packages (currently: `tidyr`, `sf`, `data.table`) STABLE!!!
-   Valuable for people new to tracking data but also has benefits for more experienced users




## 2.0 Best practices

-   Data structure
-   Meta data file (Deployment dates, central place)
-   Tidy data
-   Don't open in excel, especially RAW data files
-   Setting up R project, good structure and setting file paths
-   Standardizing the format for sharing, output formats, mention tracking databases, e.g. movebank, seabird tracking data base



## 3.0 Overview of the cleaning structure

-   How we go from RAW data files to clean data set for analysis
-   Add the cleaning mermaid workflow in as a figure
-   Quickly run through all of the steps and what the main outputs are
-   Sub-sections for each step but briefly using a single data set
-   How to use the user inputs sections
-   Point the user to the markdown where there is more detail


## 4.0 Using the App for sensivity analysis

-   The app is mainly for sensitivity analysis to understand how the processing and filtering steps effect your data set
-

## 5.0 How to make the most of the recources

-   Using GitHub, forking/downloading repository
-   Updating user inputs after sensitivity analysis
-   Using the knitted markdown file



